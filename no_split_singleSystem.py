import concurrent
import datetime
import json
import sys
import warnings
import cv2
import numpy as np
import torch.nn as nn
import torchvision.models as models
import torch
import torchvision.transforms as transforms

from json import JSONEncoder
from threading import Thread
from torch.nn.modules.container import Sequential
from PIL import Image
from tornado import httpclient, ioloop
from tornado.concurrent import Future
from tornado.queues import Queue
from utils import Config, NumpyArrayEncoder, Logger, my_models


# Ignore the warnings generated by the code.
warnings.filterwarnings('ignore')

# Read the configurations from the config file.
config = Config.get_config()

# Assign the configurations to the global variables.
device = config['client_device']
model_name = config['model']
split_point = config['split_point']
url = config['url']
frames_to_process = config['frames_to_process']

# Initialize queue for storing the output of the frames that were processed on the client (left) side.
q = Queue(maxsize=2)  

# Initialize the start time to None. This value will be set in main_runner when it is initialized.
start_time = None

# Initialize consumer start time to None. This value will be set in consumer when http request is sent to server.
consumer_startTime = None

# Initialize frame_count. This variable will be used for tracking the number of frames processed.
frame_count = 1

# Consumer frame count. This value might be different from frame_count as the consumer runs asynchronously.
consumer_frame_count = 1

# Track total responses handled.
total_handled_responses = 0


torch_utils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_convnets_processing_utils')

my_model = my_models.get(model_name, lambda: print(f"Model not present in my_models: {model_name}"))(pretrained=True)

sub_models=[my_model]

for sub_model in sub_models:
    all_modules = []

    for module in sub_model.children():
        if isinstance(module,Sequential):
            for m in module.children():
                all_modules.append(m)
        else:
            all_modules.append(module) 



all_modules = nn.Sequential(*all_modules)
all_modules = list(all_modules.children())[:21]

resnet_all = nn.Sequential(*all_modules)
resnet_all = resnet_all.eval().to(device)


async def consumer():

    global consumer_frame_count
    
    consumer_startTime = datetime.datetime.now()

    Logger.log(f'[Inside consumer] CONSUMER START TIME.')

    async for item in q:

        try:      
            consumer_frame_count += 1      
            result = item.detach().numpy()
            result = torch.Tensor(result)

            with torch.no_grad():
                output = torch.nn.functional.softmax(result, dim=1)           

        finally:
            q.task_done()

    consumer_endTime = datetime.datetime.now()

    time = (consumer_endTime - start_time).total_seconds()

    if consumer_frame_count == frames_to_process:
        Logger.log(f'TOTAL TIME FOR PROCESSING:: {time} sec')

            

def producer_video(img):
    
    tensor = convert_image_to_tensor(img)

    local_start_time = datetime.datetime.now()

    Logger.log(f'[Inside producer_video_left] [FRAME: {frame_count}] Start time of frame processing in left side.')

    output = resnet_all(tensor)
    output = output.cpu()

    local_end_time = datetime.datetime.now()

    Logger.log(f'[Inside producer_video_left] [FRAME: {frame_count}] End time of frame processing in left side.')

    Logger.log(f'[Inside producer_video_left] [FRAME: {frame_count}] Total time taken to process frame in left side:: {local_end_time - local_start_time}')
    
    return output

def convert_image_to_tensor(img):

    local_start_time = datetime.datetime.now()

    img_rgb = Image.fromarray(img).convert('RGB')
    resize = transforms.Resize([224, 224])
    img_rgb = resize(img_rgb)

    to_tensor = transforms.ToTensor()
    tensor = to_tensor(img_rgb)
    tensor = tensor.unsqueeze(0)
    tensor = tensor.to(device)
    local_end_time = datetime.datetime.now()

    Logger.log(f'[Inside convert_image_to_tensor] [FRAME: {frame_count}] Time taken to convert frame to tensor and transfer it to device:: {local_end_time - local_start_time} ')

    return tensor


async def main_runner():

    global start_time
    global frame_count
      
    # This is the start of the video processing. Initialize the start time.
    start_time = datetime.datetime.now()

    Logger.log(f'[Inside main_runner] Start time of video processing.')

    # Read the input from the file.
    cam = cv2.VideoCapture('hdvideo.mp4')


    while frame_count <= frames_to_process:

        Logger.log(f'[Inside main_runner] [FRAME: {frame_count}] Current reading frame')

        # Reading next frame from the input.       
        ret, img_rbg = cam.read()   

        # If the frame exists
        if ret: 

            # Send the frame for left processing.

            Logger.log(f'[Inside main_runner] [FRAME: {frame_count}] Sending frame for left processing')

            out_left = producer_video(img_rbg)

            Logger.log(f'[Inside main_runner] [FRAME: {frame_count}] Received frame after left processing')

            await q.put(out_left)

        # Increment frame count after left processing.    
        frame_count += 1
    
    # This is the end of the left processing. Set the end time of left video processing.
    end_time = datetime.datetime.now()

    print(f'[Inside main_runner] TOTAL TIME TAKEN FOR LEFT PROCESSING {frames_to_process} frames:: {end_time - start_time}')

    cam.release()
    cv2.destroyAllWindows()


if __name__=='__main__':

    print('Initialize IOLoop')
    io_loop = ioloop.IOLoop.current()

    print('Add main_runner and consumer to Tornado event loop as call back')
    io_loop.add_callback(main_runner)
    io_loop.add_callback(consumer) 

    print('Join the queue')
    q.join()                # Block until all the items in the queue are processed.

    print('Start IOLoop')
    io_loop.start()

    print('After start IOLoop')
    